# Initial classification model using CNN (Not used in the end)
# -*- coding: utf-8 -*-
"""nft_CS5425 W_O ID.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19SyMlf6oRlSrhW1Hytdi_Xw8ms4cIszT
"""

# from google.colab import drive
# drive.mount('/content/drive')

#Reading the training images from the path and labelling them into the given categories
import numpy as np
from PIL import Image
from numpy import asarray
import pandas as pd
import matplotlib.pyplot as plt
import cv2 # this is an important module to get imported which may even cause issues while reading the data if not used
import os
import seaborn as sns # for data visualization 
import tensorflow as tf
import os
from tensorflow.python.keras.models import Sequential #sequential api for sequential model 
from tensorflow.python.keras.layers import Dense, Dropout, Flatten #importing different layers 
from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Activation, Input, LeakyReLU,Activation
from tensorflow.python.keras import backend as K
from tensorflow.python.keras.utils import to_categorical #to perform one-hot encoding 
from tensorflow.python.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D
from tensorflow.python.keras.optimizers import RMSprop,Adam #optimiers for optimizing the model
from tensorflow.python.keras.callbacks import EarlyStopping  #regularization method to prevent the overfitting
from tensorflow.python.keras.callbacks import ModelCheckpoint
from tensorflow.python.keras.models import Sequential, Model
from tensorflow.python.keras import losses, optimizers
# from google.colab.patches import cv2_imshow
from PIL import Image
from keras.preprocessing import image
from tensorflow import keras 
from tensorflow.python.keras.models import load_model
from tensorflow.python.keras import models
from tensorflow.python.keras import backend
from tensorflow.python.keras import metrics as metrics_module

DATADIR = "/content/Assets/Training"

import os
DIR = "/content/Assets/Training"

# if you want to list all the contents in DIR
CATEGORIES = [entry for entry in os.listdir(DIR)]

IMG_SIZE= 150  
training_data = []
def create_training_data():
    for category in CATEGORIES:                                                # Looping over each category from the CATEGORIES list
        path = os.path.join(DATADIR,category)                                  # Joining images with labels
        class_num = category                                    
        for img in os.listdir(path):                                        
          img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE) 
          if img_array is None: print(class_num); print(img);  print(path);   continue 
          else:                                                                          # Converting image to greyscale to reduce the complexity and computation 
            new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE)) 
            training_data.append([new_array,class_num])                                   # Appending both the images and labels
create_training_data()
print(training_data)

# Here we will be using a user defined function create_testing_data() to extract the images from the directory
testing_data = []

import os
DATA_TEST_DIR = "/content/Assets/Testing"

TEST_CATEGORIES = [entry for entry in os.listdir(DATA_TEST_DIR)]

print(TEST_CATEGORIES)

testing_data = []
def create_testing_data():
    for category in TEST_CATEGORIES:                                                # Looping over each category from the CATEGORIES list
        path = os.path.join(DATA_TEST_DIR,category)                                  # Joining images with labels
        class_num = category                                                   
        for img in os.listdir(path):                                           
          img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE) 
          if img_array is None: print(class_num); print(img);  print(path);  continue                                              # Converting image to greyscale to reduce the complexity and computation 
          new_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))                       # Appending both the images and labels
          testing_data.append([new_array,class_num])
create_testing_data()

# Creating 4 different lists to store the image names for each category by reading them from their respective directories. 
frstlist =  [fn for fn in os.listdir(f'{DATADIR}/{CATEGORIES[0]}') ]        
scndlist  = [fn for fn in os.listdir(f'{DATADIR}/{CATEGORIES[1]}')]    
thirdlist  = [fn for fn in os.listdir(f'{DATADIR}/{CATEGORIES[2]}')]
fourthlist  = [fn for fn in os.listdir(f'{DATADIR}/{CATEGORIES[3]}')]
fifthlist  = [fn for fn in os.listdir(f'{DATADIR}/{CATEGORIES[4]}')]
sixthlist  = [fn for fn in os.listdir(f'{DATADIR}/{CATEGORIES[5]}')]
seventhlist  = [fn for fn in os.listdir(f'{DATADIR}/{CATEGORIES[6]}')]

print(CATEGORIES)
# Ranodmly selecting 3 images from each category
select_first = np.random.choice(frstlist, 3, replace = False)               
select_second = np.random.choice(scndlist, 3, replace = False)
select_third = np.random.choice(thirdlist, 3, replace = False)               
select_fourth = np.random.choice(fourthlist, 3, replace = False)
select_fifth = np.random.choice(fifthlist, 3, replace = False)               
select_sixth = np.random.choice(sixthlist, 3, replace = False)
select_seventh = np.random.choice(seventhlist, 3, replace = False)

# plotting 7 x 3 image matrix
fig = plt.figure(figsize = (12,12))

# Plotting three images from each of the four categories by looping through their path 
for i in range(21):
    if i < 3:
        fp = f'{DATADIR}/{CATEGORIES[0]}/{select_first[i]}' 
        image_name = os.path.basename(fp)                                                           
        label = CATEGORIES[0]                                            
    if i>=3 and i<6:
        fp = f'{DATADIR}/{CATEGORIES[1]}/{select_second[i-3]}'                 
        image_name = os.path.basename(fp)   
        label = CATEGORIES[1]
    if i>=6 and i<9:
        fp = f'{DATADIR}/{CATEGORIES[2]}/{select_third[i-6]}'                 
        image_name = os.path.basename(fp)   
        label = CATEGORIES[2]
    if i>=9 and i<12:
        fp = f'{DATADIR}/{CATEGORIES[3]}/{select_fourth[i-9]}'              
        image_name = os.path.basename(fp)   
        label = CATEGORIES[3]
    if i>=12 and i<15:
        fp = f'{DATADIR}/{CATEGORIES[4]}/{select_fifth[i-12]}'        
        image_name = os.path.basename(fp)   
        label = CATEGORIES[4]
    if i>=15 and i<18:
        fp = f'{DATADIR}/{CATEGORIES[5]}/{select_sixth[i-15]}'       
        image_name = os.path.basename(fp)   
        label = CATEGORIES[5]
    if i>=18 and i<21:
        fp = f'{DATADIR}/{CATEGORIES[6]}/{select_seventh[i-18]}'   
        image_name = os.path.basename(fp)   
        label = CATEGORIES[6]
   
    ax = fig.add_subplot(7, 3, i+1)
    
    # Plotting each image using load_img function
    fn = image.load_img(fp, target_size = (150,150), color_mode='grayscale')
    plt.imshow(fn, cmap='Greys_r')
    plt.title(label)
    plt.axis('off')
plt.show()

# Creating two different lists to store the Numpy arrays and the corresponding labels
X_train = []                                                                   
y_train = []
np.random.shuffle(training_data)                                               # Shuffling data to reduce variance and making sure that model remains general and overfit less
for features,label in training_data:                                           # Iterating over the training data which is generated from the create_training_data() function 
    X_train.append(features)                                                   # Appending images into X_train
    y_train.append(label)                                                     # Appending labels into y_train
print(y_train)

# Creating two different lists to store the Numpy arrays and the corresponding labels
X_test = []
y_test = []

np.random.shuffle(testing_data)                                                # Shuffling data to reduce variance and making sure that model remains general and overfit less
for features,label in testing_data:                                            # Iterating over the training data which is generated from the create_testing_data() function
    X_test.append(features)                                                    # Appending images into X_train
    y_test.append(label)  
print(y_test)                                                      # Appending labels into y_train

# Converting the list into DataFrame
y_train = pd.DataFrame(y_train, columns=["Label"],dtype=object) 
y_test = pd.DataFrame(y_test, columns=["Label"],dtype=object)
print(y_train)
print(y_test)

# Storing the value counts of target variable
imageType_count=y_train.Label.value_counts()
print(imageType_count)
print('*'*10)
imageType_count=y_train.Label.value_counts(normalize=True)
print(imageType_count)

# Converting the pixel values into Numpy array
X_train= np.array(X_train) 
X_test= np.array(X_test) 
print(X_train)
print(X_test)

X_train= X_train/255.0
X_test = X_test/255.0

from sklearn.preprocessing import LabelBinarizer
# Storing the LabelBinarizer function in lb variable
lb = LabelBinarizer() 
# Applying fit_transform on train target variable
y_train_e = lb.fit_transform(y_train)
print(y_train_e)
# Applying only transform on test target variable
y_test_e = lb.fit_transform(y_test)

from tensorflow.python.keras import backend
backend.clear_session()
#Fixing the seed for random number generators so that we can ensure we receive the same output everytime
np.random.seed(42)
import random
random.seed(42)
tf.random.set_seed(42)

# initialized a sequential model
model_2 = Sequential()

# adding first conv layer with 256 filters and kernel size 5x5 , with ReLU activation and padding 'same' provides the output size same as the input size
#input_shape denotes input image dimension of images
model_2.add(Conv2D(filters = 256, kernel_size = (5,5),padding = 'Same', 
                 activation ='relu', input_shape = (150,150,1)))  # Shape of the image given to the model

# adding max pooling to reduce the size of output of first conv layer
model_2.add(MaxPool2D(pool_size=(2,2)))
#  adding dropout to randomly switch off 25% neurons to reduce overfitting
model_2.add(Dropout(0.25))

#  adding second conv layer with 128 filters and with kernel size 3x3 and ReLu activation function
model_2.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', 
                 activation ='relu'))
# adding max pooling havinf pool_size and strides with (2,2) to reduce the size of output of second conv layer
model_2.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
#  adding dropout to randomly switch off 25% neurons to reduce overfitting
model_2.add(Dropout(0.25))

#  adding third conv layer with 128 filters and with kernel size 3x3 and ReLu activation function
model_2.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', 
                 activation ='relu'))
# adding max pooling havinf pool_size and strides with (2,2) to reduce the size of output of third conv layer
model_2.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
#  adding dropout to randomly switch off 30% neurons to reduce overfitting
model_2.add(Dropout(0.3))

# adding fourth conv layer with 128 filters and with kernel size 2x2 and ReLu activation function
model_2.add(Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', 
                 activation ='relu'))
# adding max pooling havinf pool_size and strides with (2,2) to reduce the size of output of fourth conv layer
model_2.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
#  adding dropout to randomly switch off 30% neurons to reduce overfitting
model_2.add(Dropout(0.3))

#adding fifth conv layer with 128 filters and with kernel size 2x2 and ReLu activation function
model_2.add(Conv2D(filters = 128, kernel_size = (2,2),padding = 'Same', 
                 activation ='relu'))
# adding max pooling havinf pool_size and strides with (2,2) to reduce the size of output of fifth conv layer
model_2.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))
#  adding dropout to randomly switch off 30% neurons to reduce overfitting
model_2.add(Dropout(0.3))

# flattening the 3-d output of the conv layer after max pooling to make it ready for creating dense connections
model_2.add(Flatten())
# adding a fully connected dense layer with 1024 neurons 
model_2.add(Dense(1024, activation = "relu"))
#  adding dropout to randomly switch off 50% neurons to reduce overfitting
model_2.add(Dropout(0.5))
# adding the output layer with 4 neurons and activation functions as softmax since this is a multi-class classification problem.
model_2.add(Dense(7, activation = "softmax"))

# printing the model summary
model_2.summary()

# Taking Adam as an optimizer with learning rate 0.001
optimizer = Adam(lr=0.001)
# Compiling the model with Adam as optimizer, categorical_crossentropy as loss function and accuracy as metrics
model_2.compile(optimizer = optimizer , loss = "categorical_crossentropy", metrics=["accuracy"])
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)
mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)
# Fitting the model with 30 epochs and validation_split as 20%
history=model_2.fit(X_train, 
          y_train_e,  
          epochs=30, 
          batch_size=64,validation_split=0.20,callbacks=[es, mc])

# printing the model summary
model_2.summary()

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Test Prediction 
y_test_pred_ln2 = model_2.predict(X_test)
y_test_pred_classes_ln2 = np.argmax(y_test_pred_ln2, axis=1)
normal_y_test = np.argmax(y_test_e, axis=1)

import seaborn as sns
from sklearn.metrics import accuracy_score, confusion_matrix
accuracy_score((normal_y_test), y_test_pred_classes_ln2)

pred=model_2.predict(X_test)
#printing the first element from predicted data
print(pred[0])
#printing the index of 
print('Index:',np.argmax(pred[0]))

y_classes = [np.argmax(element) for element in pred]
print('Predicted_values:',y_classes[:20])
print('Actual_values:',y_test[:20])

res=model_2.predict(X_test[4].reshape(1,150,150))
plt.imshow(X_test[4].reshape(150,150), cmap='Greys_r')
# i1=0
# for i in X_test:
#   # print(i1 )
#   print(i1,y_test.Label[i1])
#   plt.imshow(X_test[i1].reshape(150,150), cmap='Greys_r')
#   i1 = i1+1
# plt.imshow(X_test[1].reshape(150,150), cmap='Greys_r')
# #i=y_test.Label[5]
#i=np.argmax(i)
# i=y_test.Label[1]
i=np.argmax(res)
print(i)
print(y_test.Label[i])
print(y_test.Label)

model_2.evaluate(X_test,y_test_e)

# Test Prediction 
y_test_pred_ln2 = model_2.predict(X_test)
y_test_pred_classes_ln2 = np.argmax(y_test_pred_ln2, axis=1)
normal_y_test = np.argmax(y_test_e, axis=1)

# Test Accuracy 
import seaborn as sns
from sklearn.metrics import accuracy_score, confusion_matrix
accuracy_score((normal_y_test), y_test_pred_classes_ln2)

cf_matrix = confusion_matrix(normal_y_test, y_test_pred_classes_ln2)

# Confusion matrix normalized per category true value
cf_matrix_n1 = cf_matrix/np.sum(cf_matrix, axis=1)
plt.figure(figsize=(8,6))
sns.heatmap(cf_matrix_n1, xticklabels=CATEGORIES, yticklabels=CATEGORIES, annot=True)

filepath = '/content/34.png'

from PIL import Image
img = Image.open(filepath)
print(img)

numpydata = asarray(img)
print(numpydata)

print(type(numpydata))

print(numpydata.shape)
img22 = cv2.imread(os.path.join(filepath),cv2.IMREAD_GRAYSCALE)
new_array12 = cv2.resize(img22,(IMG_SIZE,IMG_SIZE)) 
plt.imshow(img22)
plt.show()
data1=[]
data1.append(new_array12)
X_train1= np.array(data1) 
res=model_2.predict(X_train1[0].reshape(1,150,150))
plt.imshow(X_train1[0].reshape(150,150), cmap='Greys_r')
i=np.argmax(res)
print(i)
print(y_test.Label[i])